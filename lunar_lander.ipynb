{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f068bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f84cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = [3, 3, 5.1, 5.1, 3.3, 5.1, 1, 1] \n",
    "mini = [-1.7, -1.7, -5.1, -5.1, -3.3, -5.1, 0, 0]\n",
    "my_bin = [5, 5, 5, 5, 5, 5, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7820bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = [1.5, 1.5, 5, 5, 3.14, 5, 1, 1] \n",
    "mini = [-1.5, -1.5, -5, -5, -3.14, -5, 0, 0]\n",
    "my_bin = [10, 10, 10, 10, 10, 10, 2, 2]\n",
    "#my_bin = [5, 5, 5, 5, 5, 5, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170053d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff062454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation = env.reset(seed=20, return_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f86bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([-1.6,  1.3999956 , -0.3203935 , -0.48554128,  0.00367194,\n",
    "        0.072574  ,  0.        ,  0.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8508785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(obs):\n",
    "    \n",
    "    obs = [min(elem_maxi, elem_obs) for elem_obs, elem_maxi in zip(obs, maxi)]\n",
    "    \n",
    "    obs = [max(elem_mini, elem_obs) for elem_obs, elem_mini in zip(obs, mini)]\n",
    "    \n",
    "    res = np.int64(np.floor(0.99*np.array(my_bin)*(obs-np.array(mini))/(np.array(maxi)-np.array(mini))))\n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d12477",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =get_discrete_state(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17597f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69501f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6f7e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10, 10, 10, 10, 2, 2, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(my_bin + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4a0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.15\n",
    "\n",
    "DISCOUNT = 0.995\n",
    "EPISODES = 2000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "\n",
    "epsilon = 1 # Taux d'apprentissage\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f096a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab2a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Average: 1.8765689531962078\n",
      "Mean Reward: -6.820933918175267\n",
      "58\n",
      "1\n",
      "73\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14222/2753966311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#q_table[discrete_state + (action,)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mnew_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_q\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_future_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "rewards = []\n",
    "cpt_step = 0\n",
    "for episode in range(EPISODES + 1): #go through the episodes\n",
    "    print(cpt_step)\n",
    "    cpt_step = 0\n",
    "    print(episode)\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) # get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 # reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "        \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) # do a random action\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) # step action to get new states, reward, and the \"done\" status.\n",
    "        cpt_step += 1\n",
    "        episode_reward += reward # add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: # render\n",
    "            env.render()\n",
    "\n",
    "        if not done: # update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[np.array(list(discrete_state) + [action])]\n",
    "            \n",
    "            #q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[np.array(list(discrete_state) + [action])] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.01: # epsilon modification\n",
    "       # if episode_reward > prior_reward and episode > 10000: \n",
    "        if episode > 10000:\n",
    "            epsilon *= 1 - math.pow(5, -6);\n",
    "            #epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() # episode has finished\n",
    "    episode_total = t1 - t0 # episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward # episode total reward\n",
    "    prior_reward = episode_reward\n",
    "    rewards.append(episode_reward)\n",
    "    if episode % 3 == 0: # every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 3\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 3\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7989c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/numexpr/expressions.py:21: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _np_version_forbids_neg_powint = LooseVersion(numpy.__version__) >= LooseVersion('1.12.0b1')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c440001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f62886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewards</th>\n",
       "      <th>discount</th>\n",
       "      <th>learning rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-20.462802</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-83.543683</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rewards  discount  learning rate\n",
       "0 -20.462802     0.995           0.15\n",
       "1 -83.543683     0.995           0.15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'rewards': rewards, 'discount' : DISCOUNT, 'learning rate' : LEARNING_RATE}).to_csv('rewards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe5ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5afef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
