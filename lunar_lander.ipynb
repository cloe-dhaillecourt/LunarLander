{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f068bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a29481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = [3, 3, 5.1, 5.1, 3.3, 5.1, 1, 1] \n",
    "mini = [-1.7, -1.7, -5.1, -5.1, -3.3, -5.1, 0, 0]\n",
    "my_bin = [5, 5, 5, 5, 5, 5, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5549bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = [1.5, 1.5, 5, 5, 3.14, 5, 1, 1] \n",
    "mini = [-1.5, -1.5, -5, -5, -3.14, -5, 0, 0]\n",
    "my_bin = [10, 10, 10, 10, 10, 10, 2, 2]\n",
    "my_bin = [5, 5, 5, 5, 5, 5, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f91f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff062454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation = env.reset(seed=20, return_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3427cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([-1.6,  1.3999956 , -0.3203935 , -0.48554128,  0.00367194,\n",
    "        0.072574  ,  0.        ,  0.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f1946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(obs):\n",
    "    \n",
    "    obs = [min(elem_maxi, elem_obs) for elem_obs, elem_maxi in zip(obs, maxi)]\n",
    "    \n",
    "    obs = [max(elem_mini, elem_obs) for elem_obs, elem_mini in zip(obs, mini)]\n",
    "    \n",
    "    res = np.int64(np.floor(0.99*np.array(my_bin)*(obs-np.array(mini))/(np.array(maxi)-np.array(mini))))\n",
    "    return res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de9388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =get_discrete_state(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36269adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6e543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d74c2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 5, 5, 5, 5, 2, 2, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(my_bin + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb80fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03ddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.15\n",
    "\n",
    "DISCOUNT = 0.995\n",
    "EPISODES = 50\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "\n",
    "epsilon = 1 # Taux d'apprentissage\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e738e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f096a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab2a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloedhaillecourt/anaconda3/lib/python3.9/site-packages/gym/core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode: 0\n",
      "Time Average: 0.0013513729572296144\n",
      "Mean Reward: -0.12137672355703688\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "rewards = []\n",
    "for episode in range(EPISODES + 1): #go through the episodes\n",
    "    \n",
    "    print(episode)\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) # get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 # reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "        \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) # do a random action\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action) # step action to get new states, reward, and the \"done\" status.\n",
    "\n",
    "        episode_reward += reward # add the reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if episode % 2000 == 0: # render\n",
    "            env.render()\n",
    "\n",
    "        if not done: # update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[np.array(list(discrete_state) + [action])]\n",
    "            \n",
    "            #q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[np.array(list(discrete_state) + [action])] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if epsilon > 0.01: # epsilon modification\n",
    "       # if episode_reward > prior_reward and episode > 10000: \n",
    "        if episode > 10000:\n",
    "            epsilon *= 1 - math.pow(5, -6);\n",
    "            #epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    t1 = time.time() # episode has finished\n",
    "    episode_total = t1 - t0 # episode total time\n",
    "    total = total + episode_total\n",
    "\n",
    "    total_reward += episode_reward # episode total reward\n",
    "    prior_reward = episode_reward\n",
    "    rewards.append(episode_reward)\n",
    "    if episode % 1000 == 0: # every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15214b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
